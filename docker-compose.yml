version: "3"
services:

  libre-chat:
    build: .
    # image: ghcr.io/vemonet/libre-chat:main
    volumes:
      # - ./config/chat-vectorstore-qa.yml:/data/chat.yml
      - ./chat.yml:/data/chat.yml
      - ./data/models:/data/models
      - ./data/documents:/data/documents
      - ./data/embeddings:/data/embeddings
      - ./data/vectorstore:/data/vectorstore
      # - ./data:/data # Or directly share the data directory with chat.yml, and folders for models, vectorstore, etc
    shm_size: '16g'
    environment:
      - LIBRECHAT_WORKERS=1
      # For deployment with nginx-proxy https://github.com/nginx-proxy/nginx-proxy
      - VIRTUAL_HOST=chat.semanticscience.org
      - LETSENCRYPT_HOST=chat.semanticscience.org
      - VIRTUAL_PORT=8000
      # - CUDA_VISIBLE_DEVICES=0 # Limit which GPU is made available
    # ports:
    #   - 8000:8000
    # entrypoint: uvicorn scripts.main:app --reload
    deploy:  # Enable GPU in the container
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 3
              capabilities: [gpu]

    # networks:
    #   - no-internet

# networks:
#   no-internet:
#     driver: bridge
#     internal: true
